llm_config:
  model: ./data/model/aeon-360M-Q8_0.gguf
  temperature: 0.2
  n_ctx: 2048
  top_k: 40
  top_p: 0.8
  llm_prompt: >
    ANSWER the user QUESTION using ONLY the CONTEXT.
    Keep your ANSWER ground in the facts of the CONTEXT.
    If the CONTEXT doesn't contain the facts to ANSWER the QUESTION state: I dont know, can we /search?

  llm_rag_prompt: >
    "Your responses should be in plain, natural language ONLY.
    Determine the nature of the user's QUESTION and USER.
    If the question is factual, follow this process:
    1. Scan the CONTEXT for all relevant facts.
    2. Combine these facts to form a single, comprehensive answer.
    3. If context is unavailable, state: 'I don't know about it. Can we /search?'.
    If the question is conversational or non-factual, respond naturally and conversationally, without referring to the CONTEXT.
    Do not echo the user's USER, QUESTION or the CONTEXT.

emb_config:
  model: ./data/model/all-MiniLM-L6-v2-Q8_0.gguf
  n_ctx: 512
  chunk_size: 100
  chunk_overlap: 50

load_plugins:
  - aeon-speak
  - hello-world
  - aeon-smolvlm-256m-instruct
  - aeon-tiny-sd