llm_config:
  model: ./data/model/aeon-360M_Q4.gguf
  temperature: 0.2
  n_ctx: 2048
  top_k: 40
  top_p: 0.8
  llm_prompt: >
    ANSWER the users QUESTION using ONLY the CONTEXT.
    Keep your ANSWER ground in the facts of the CONTEXT.
    If the CONTEXT doesnâ€™t contain the facts to ANSWER the QUESTION state: I dont know, can we /search?

  llm_rag_prompt: >
    "Your responses should be in plain, natural language ONLY.
    Determine the nature of the user's QUESTION.
    If the question is factual, follow this process:
    1. Scan the CONTEXT for all relevant facts.
    2. Combine these facts to form a single, comprehensive answer.
    3. If context is unavailable,
    state: 'I don't know about it. Can we /search?'.
    If the question is conversational
    or non-factual, respond naturally and
    conversationally, without referring to the CONTEXT.
    Do not echo the user's QUESTION or the CONTEXT.

emb_config:
  model: ./data/model/all-MiniLM-L6-v2-Q8_0.gguf
  n_ctx: 256
  chunk_size: 20
  chunk_overlap: 0

load_plugins: 
  - aeon
  - smolvlm-256m-instruct
  - tiny-sd 
  - hello-world